{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Engineer Capstone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11088d929a0a4a7aae2f478bedefbe5c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93300d027a7e473fa67683f6c9330480"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3292e9236e624bbebd221f589a474a2d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34e750489ba743ec97c18b667e430e17"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad2e71af7b62416a89dfc876e79199fe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import glob\n",
    "import subprocess\n",
    "import json\n",
    "import os\n",
    "import csv\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input\n",
    "import numpy as np\n",
    "\n",
    "# taken from https://github.com/harvitronix/five-video-classification-methods\n",
    "class Extractor():\n",
    "    def __init__(self, weights=None):\n",
    "        \"\"\"Either load pretrained from imagenet, or load our saved\n",
    "        weights from our own training.\"\"\"\n",
    "\n",
    "        self.weights = weights  # so we can check elsewhere which model\n",
    "\n",
    "        if weights is None:\n",
    "            # Get model with pretrained weights.\n",
    "            base_model = InceptionV3(\n",
    "                weights='imagenet',\n",
    "                include_top=True\n",
    "            )\n",
    "\n",
    "            # We'll extract features at the final pool layer.\n",
    "            self.model = Model(\n",
    "                inputs=base_model.input,\n",
    "                outputs=base_model.get_layer('avg_pool').output\n",
    "            )\n",
    "\n",
    "        else:\n",
    "            # Load the model first.\n",
    "            self.model = load_model(weights)\n",
    "\n",
    "            # Then remove the top so we get features not predictions.\n",
    "            # From: https://github.com/fchollet/keras/issues/2371\n",
    "            self.model.layers.pop()\n",
    "            self.model.layers.pop()  # two pops to get to pool layer\n",
    "            self.model.outputs = [self.model.layers[-1].output]\n",
    "            self.model.output_layers = [self.model.layers[-1]]\n",
    "            self.model.layers[-1].outbound_nodes = []\n",
    "\n",
    "    def extract(self, image_path):\n",
    "        img = image.load_img(image_path, target_size=(299, 299))\n",
    "        x = image.img_to_array(img)\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = preprocess_input(x)\n",
    "\n",
    "        # Get the prediction.\n",
    "        features = self.model.predict(x)\n",
    "\n",
    "        if self.weights is None:\n",
    "            # For imagenet/default network:\n",
    "            features = features[0]\n",
    "        else:\n",
    "            # For loaded network:\n",
    "            features = features[0]\n",
    "\n",
    "        return features\n",
    "\n",
    "# returns the number of secs in video\n",
    "def video_length(path):\n",
    "    cmd = \"ffprobe -i \" + path + \" -show_entries format=duration -v quiet -of json\"\n",
    "    pipe = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE).stdout\n",
    "    output = pipe.read()\n",
    "    d = json.loads(output)\n",
    "    s = d[\"format\"][\"duration\"]\n",
    "    return int(float(s))\n",
    "\n",
    "# returns the id of a video in the ./data/videos dir\n",
    "def video_id(path):\n",
    "    return path.split(\"/\")[3].split(\".\")[0]\n",
    "\n",
    "def clip_dir_path(path):\n",
    "    vid_id = video_id(path)\n",
    "    return \"./data/clips/\" + vid_id\n",
    "\n",
    "# creates a folder with one sec clips from the source video\n",
    "# takes about 30 mins for a 20 min video\n",
    "def create_clips(path):\n",
    "    # create clip dir\n",
    "    dir_path = clip_dir_path(path)\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    \n",
    "    # create one sec clips from src\n",
    "    video_len = video_length(path)\n",
    "    for i in tqdm_notebook(xrange(video_len), desc=\"Clips for \" + video_id(path)):\n",
    "        clip_path = dir_path + \"/\" + '%05d' % i + \".mp4\"    \n",
    "        if not os.path.exists(clip_path):\n",
    "            cmd = \"ffmpeg -v error -y -i \" + path + \" -ss \" + str(i) + \" -t 1 \" + clip_path\n",
    "            os.system(cmd)\n",
    "\n",
    "# creates folders with frames for each clip of source video\n",
    "def create_frames(path):\n",
    "    # create frame dir\n",
    "    vid_id = video_id(path)\n",
    "    dir_path = \"./data/frames/\" + vid_id\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    \n",
    "    # create frames from clip\n",
    "    video_len = video_length(path)\n",
    "    for i in tqdm_notebook(xrange(video_len), desc=\"Frames for \" + vid_id):\n",
    "        clip_path = clip_dir_path(path) + \"/\" + '%05d' % i + \".mp4\"\n",
    "        frame_dir_path = dir_path + \"/\" + '%05d' % i\n",
    "        if not os.path.exists(frame_dir_path):\n",
    "            os.makedirs(frame_dir_path)\n",
    "            cmd = \"ffmpeg -v error -y -i \" + clip_path + \" -r 5.0 \" + frame_dir_path + \"/%5d.jpg\"\n",
    "            os.system(cmd)\n",
    "            \n",
    "            # resize frames to 299x299 for InceptionV3\n",
    "            frame_paths = glob.glob(frame_dir_path + \"/*.jpg\")\n",
    "            for fi in xrange(len(frame_paths)):\n",
    "                path = frame_paths[fi]\n",
    "                # resize first\n",
    "                cmd = \"convert \" + path + \" -resize 299x299 \" + path\n",
    "                os.system(cmd)\n",
    "                # add black background\n",
    "                cmd = \"convert \" + path + \" -gravity center -background black -extent 299x299 \" + path\n",
    "                os.system(cmd)\n",
    "\n",
    "def create_spectrograms(path):\n",
    "    # create audio dir\n",
    "    vid_id = video_id(path)\n",
    "    dir_path = \"./data/audio/\" + vid_id\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "    \n",
    "    # create spectrogram from clip\n",
    "    video_len = video_length(path)\n",
    "    for i in tqdm_notebook(xrange(video_len), desc=\"Spectrograms for \" + vid_id):\n",
    "        clip_path = clip_dir_path(path) + \"/\" + '%05d' % i + \".mp4\"\n",
    "        spec_path = dir_path + \"/\" + '%05d' % i + \".png\"\n",
    "        if not os.path.exists(spec_path):\n",
    "            cmd = \"ffmpeg -v error -y -i \" + clip_path + \" -lavfi showspectrumpic=s=32x32:legend=false \" + spec_path\n",
    "            os.system(cmd)\n",
    "\n",
    "\n",
    "extractor = Extractor()\n",
    "\n",
    "def create_features(path):\n",
    "    # create feature dir\n",
    "    vid_id = video_id(path)\n",
    "    dir_path = \"./data/features/\" + vid_id\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)    \n",
    "    \n",
    "    video_len = video_length(path)    \n",
    "    with tqdm_notebook(total=video_len, desc=\"Features for \" + vid_id) as pbar:\n",
    "        for root, dirs, files in os.walk('./data/frames/'+ vid_id):\n",
    "            for f in files:\n",
    "                if f.endswith(\".jpg\"):\n",
    "                    frame_path = root + \"/\" + f\n",
    "                    feature_path = frame_path.replace(\"frames\", \"features\").replace(\"jpg\", \"txt.gz\")\n",
    "                    feature_dir = root.replace(\"frames\", \"features\")\n",
    "                    features = extractor.extract(frame_path)\n",
    "                    if not os.path.exists(feature_dir):\n",
    "                        os.makedirs(feature_dir)\n",
    "                    np.savetxt(feature_path, features)\n",
    "            pbar.update(1)\n",
    "\n",
    "video_paths = glob.glob(\"./data/videos/*.mp4\")\n",
    "videos_len = len(video_paths)\n",
    "for i in tqdm_notebook(xrange(videos_len), desc=\"Preprocessing Videos\"):\n",
    "    path = video_paths[i]\n",
    "    create_clips(path)\n",
    "    create_frames(path)\n",
    "    create_spectrograms(path)\n",
    "    create_features(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels Shape: (641, 3)\n",
      "Trainging Labels Shape: (384, 3)\n",
      "Validation Labels Shape: (128, 3)\n",
      "Test Labels Shape: (129, 3)\n",
      "Sample Trainging Labels Shape: (36, 3)\n",
      "Sample Validation Labels Shape: (12, 3)\n",
      "Sample Test Labels Shape: (12, 3)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import numpy as np\n",
    "\n",
    "# read in and shuffle data\n",
    "labels = pd.read_csv(\"./labelmaker/labels.csv\").as_matrix()\n",
    "print \"Labels Shape: {}\".format(labels.shape)\n",
    "np.random.seed(0)\n",
    "np.random.shuffle(labels)\n",
    "\n",
    "# split labels into train, validation, and test sets\n",
    "div = len(labels) / 5\n",
    "train_labels = labels[0:div*3,:]\n",
    "val_labels = labels[div*3:div*4,:]\n",
    "test_labels = labels[div*4:,:]\n",
    "\n",
    "print \"Trainging Labels Shape: {}\".format(train_labels.shape)\n",
    "print \"Validation Labels Shape: {}\".format(val_labels.shape)\n",
    "print \"Test Labels Shape: {}\".format(test_labels.shape)\n",
    "\n",
    "# split labels into sample train, validation, and test sets\n",
    "smpl_div = div / 10\n",
    "smpl_train_labels = labels[0:smpl_div*3,:]\n",
    "smpl_val_labels = labels[smpl_div*3:smpl_div*4,:]\n",
    "smpl_test_labels = labels[smpl_div*4:smpl_div*5,:]\n",
    "\n",
    "print \"Sample Trainging Labels Shape: {}\".format(smpl_train_labels.shape)\n",
    "print \"Sample Validation Labels Shape: {}\".format(smpl_val_labels.shape)\n",
    "print \"Sample Test Labels Shape: {}\".format(smpl_test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Reshape' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-f9b5a1f01be9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf1_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf2_x\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlstm_input_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreturn_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Reshape' is not defined"
     ]
    }
   ],
   "source": [
    "from keras.applications import InceptionV3\n",
    "from keras.applications import Xception\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model\n",
    "from keras.layers import concatenate\n",
    "from keras.layers import Dense, GlobalAveragePooling2D, Input, Embedding, LSTM, Dropout\n",
    "from keras import backend as K\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.preprocessing.image import load_img\n",
    "\n",
    "dropout = 0.5\n",
    "\n",
    "f1_base = InceptionV3(weights='imagenet', include_top=False)\n",
    "for layer in f1_base.layers:\n",
    "    layer.trainable = False    \n",
    "f1_x = f1_base.output\n",
    "f1_x = GlobalAveragePooling2D()(f1_x)\n",
    "f1_x = Dense(1024, activation='relu')(f1_x)\n",
    "f1_x = Dropout(dropout)(f1_x)\n",
    "\n",
    "# because of https://github.com/fchollet/keras/issues/7412\n",
    "f2_base = Xception(weights='imagenet', include_top=False)\n",
    "for layer in f2_base.layers:\n",
    "    layer.trainable = False    \n",
    "f2_x = f2_base.output\n",
    "f2_x = GlobalAveragePooling2D()(f2_x)\n",
    "f2_x = Dense(1024, activation='relu')(f2_x)\n",
    "f2_x = Dropout(dropout)(f2_x)\n",
    "\n",
    "lstm_input_shape = (2, 1024)\n",
    "\n",
    "x = concatenate([f1_x, f2_x])\n",
    "x = Reshape(lstm_input_shape)(x)\n",
    "x.add(LSTM(512,return_sequences=True,dropout=0.5))\n",
    "x.add(Flatten())\n",
    "x.add(Dense(512, activation='relu'))\n",
    "x.add(Dropout(0.5))\n",
    "x.add(Dense(4, activation='softmax'))\n",
    "        \n",
    "# x = concatenate([f1_x, f2_x])\n",
    "# x = Dense(1024, activation='relu')(x)\n",
    "# x = Dropout(dropout)(x)\n",
    "# x = Dense(1024, activation='relu')(x)\n",
    "# x = Dropout(dropout)(x)\n",
    "# x = Dense(4, activation='sigmoid', name=\"main_output\")(x)\n",
    "model = Model(inputs=[f1_base.input, f2_base.input],outputs=[x])\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"sgd\", metrics=['accuracy'])\n",
    "\n",
    "print \"Model Compiled\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting Model\n",
      "Epoch 1/30\n",
      "210s - loss: 1.4025 - acc: 0.2600 - val_loss: 1.2795 - val_acc: 0.4667\n",
      "Epoch 2/30\n",
      "191s - loss: 1.3820 - acc: 0.3000 - val_loss: 1.3243 - val_acc: 0.3667\n",
      "Epoch 3/30\n",
      "189s - loss: 1.3857 - acc: 0.3500 - val_loss: 1.3530 - val_acc: 0.3667\n",
      "Epoch 4/30\n",
      "189s - loss: 1.4180 - acc: 0.2700 - val_loss: 1.3432 - val_acc: 0.3667\n",
      "Epoch 5/30\n",
      "198s - loss: 1.3876 - acc: 0.3200 - val_loss: 1.3591 - val_acc: 0.3000\n",
      "Epoch 6/30\n",
      "195s - loss: 1.3825 - acc: 0.3400 - val_loss: 1.3694 - val_acc: 0.4000\n",
      "Epoch 7/30\n",
      "200s - loss: 1.3264 - acc: 0.2900 - val_loss: 1.3377 - val_acc: 0.3333\n",
      "Epoch 8/30\n",
      "195s - loss: 1.3844 - acc: 0.3300 - val_loss: 1.3801 - val_acc: 0.3333\n",
      "Epoch 9/30\n",
      "192s - loss: 1.3494 - acc: 0.3300 - val_loss: 1.3582 - val_acc: 0.3667\n",
      "Epoch 10/30\n",
      "193s - loss: 1.3724 - acc: 0.3400 - val_loss: 1.3342 - val_acc: 0.2667\n",
      "Epoch 11/30\n",
      "194s - loss: 1.3671 - acc: 0.2600 - val_loss: 1.3160 - val_acc: 0.3667\n",
      "Epoch 12/30\n",
      "192s - loss: 1.3875 - acc: 0.3500 - val_loss: 1.3645 - val_acc: 0.4000\n",
      "Epoch 13/30\n",
      "198s - loss: 1.3714 - acc: 0.3500 - val_loss: 1.3656 - val_acc: 0.2000\n",
      "Epoch 14/30\n",
      "194s - loss: 1.3001 - acc: 0.2900 - val_loss: 1.2346 - val_acc: 0.4333\n",
      "Epoch 15/30\n",
      "195s - loss: 1.3605 - acc: 0.3800 - val_loss: 1.2985 - val_acc: 0.4333\n",
      "Epoch 16/30\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a0ad08fb8fce>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgenerate_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m                     callbacks=[tensorboard])\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"shot_classifier_\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\".h5\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python2.7/site-packages/keras/legacy/interfaces.pyc\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, initial_epoch)\u001b[0m\n\u001b[1;32m   1838\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   1839\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1840\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   1841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1563\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python2.7/site-packages/keras/backend/tensorflow_backend.pyc\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2266\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2267\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2268\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda/lib/python2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.callbacks import TensorBoard\n",
    "import time\n",
    "\n",
    "preprocess = preprocess_input\n",
    "\n",
    "def one_hot(i):\n",
    "    a = np.array([int(i==0),int(i==1),int(i==2),int(i==3)])\n",
    "    return a[None,:]\n",
    "\n",
    "def generate_images(labels):\n",
    "    while 1:\n",
    "        for i in xrange(len(labels)):\n",
    "            video_id = labels[i][0]\n",
    "            clip_id = labels[i][1]\n",
    "            label = labels[i][2]\n",
    "\n",
    "            # image 1 \n",
    "            img_1_path = \"./data/frames/\" + video_id + \"/\" + '%05d' % clip_id + \"/00001.jpg\"\n",
    "            img_1 = load_img(img_1_path, target_size=(299,299))\n",
    "            img_1 = img_to_array(img_1)\n",
    "            img_1 = np.expand_dims(img_1, axis=0)\n",
    "            img_1 = preprocess(img_1)\n",
    "\n",
    "            # image 2 \n",
    "            img_2_path = \"./data/frames/\" + video_id + \"/\" + '%05d' % clip_id + \"/00007.jpg\"\n",
    "            img_2 = load_img(img_2_path, target_size=(299,299))\n",
    "            img_2 = img_to_array(img_2)\n",
    "            img_2 = np.expand_dims(img_2, axis=0)\n",
    "            img_2 = preprocess(img_2)\n",
    "\n",
    "            yield ([img_1, img_2], one_hot(label))\n",
    "\n",
    "print \"Fitting Model\"\n",
    "tensorboard = TensorBoard(log_dir='./logs', \n",
    "                          histogram_freq=0,\n",
    "                          write_graph=True, \n",
    "                          write_images=True)\n",
    "\n",
    "model.fit_generator(generate_images(train_labels),\n",
    "                    100,\n",
    "                    epochs=30,\n",
    "                    verbose=2,\n",
    "                    validation_data=generate_images(val_labels),\n",
    "                    validation_steps=30,\n",
    "                    callbacks=[tensorboard])\n",
    "\n",
    "file_name = \"shot_classifier_\" + str(int(time.time())) + \".h5\"\n",
    "model.save(file_name)\n",
    "print \"Model Saved\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "def predict_generate_images(labels):\n",
    "    while 1:\n",
    "        for i in xrange(len(labels)):\n",
    "            video_id = labels[i][0]\n",
    "            clip_id = labels[i][1]\n",
    "            label = labels[i][2]\n",
    "\n",
    "            # image 1 \n",
    "            img_1_path = \"./data/frames/\" + video_id + \"/\" + '%05d' % clip_id + \"/00001.jpg\"\n",
    "            img_1 = load_img(img_1_path, target_size=(299,299))\n",
    "            img_1 = img_to_array(img_1)\n",
    "            img_1 = np.expand_dims(img_1, axis=0)\n",
    "            img_1 = preprocess(img_1)\n",
    "\n",
    "            # image 2 \n",
    "            img_2_path = \"./data/frames/\" + video_id + \"/\" + '%05d' % clip_id + \"/00007.jpg\"\n",
    "            img_2 = load_img(img_2_path, target_size=(299,299))\n",
    "            img_2 = img_to_array(img_2)\n",
    "            img_2 = np.expand_dims(img_2, axis=0)\n",
    "            img_2 = preprocess(img_2)\n",
    "\n",
    "            yield [img_1, img_2]\n",
    "\n",
    "model = load_model('shot_classifier.h5')\n",
    "predictions = model.predict_generator(predict_generate_images(smpl_test_labels), 10)\n",
    "for i in xrange(len(predictions)):\n",
    "    p = predictions[i]\n",
    "    print p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Read in Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "\n",
    "labels = pd.read_csv(\"./data/labels.csv\").as_matrix()\n",
    "print \"Labels Shape: {}\".format(labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# write label to image mapping file\n",
    "with open('data/audio_labels.txt', 'a') as f:\n",
    "    for label in labels:\n",
    "        vid_id = label[0]\n",
    "        clip_id = label[1]\n",
    "        value = 0 if label[2] == 0 else 1 \n",
    "        img_path = \"./data/audio/\" + vid_id + \"/\" + '%05d' % clip_id + \".png\"\n",
    "        line = img_path + \" \" + '%d' % value + \"\\n\"\n",
    "        f.write(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create Audio Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tflearn\n",
    "from tflearn.layers.core import input_data, dropout, fully_connected\n",
    "from tflearn.layers.conv import conv_2d, max_pool_2d\n",
    "from tflearn.layers.normalization import local_response_normalization\n",
    "from tflearn.layers.estimator import regression\n",
    "from tflearn.data_preprocessing import ImagePreprocessing\n",
    "from tflearn.data_utils import image_preloader\n",
    "\n",
    "# Load path/class_id image file:\n",
    "dataset_file = 'data/audio_labels.txt'\n",
    "dataset_sample = 'data/audio_labels_sample.txt'\n",
    "\n",
    "# Build the preloader array, resize images to 300x300\n",
    "from tflearn.data_utils import image_preloader\n",
    "X, Y = image_preloader(dataset_file, \n",
    "                       image_shape=(32, 32),\n",
    "                       mode='file', \n",
    "                       categorical_labels=True,   \n",
    "                       normalize=True)\n",
    "\n",
    "# Real-time data preprocessing\n",
    "img_prep = ImagePreprocessing()\n",
    "img_prep.add_featurewise_zero_center()\n",
    "img_prep.add_featurewise_stdnorm()\n",
    "\n",
    "# Building convolutional network\n",
    "network = input_data(shape=[None, 32, 32, 3],\n",
    "                     data_preprocessing=img_prep)\n",
    "network = conv_2d(network, 32, 3, activation='relu')\n",
    "network = max_pool_2d(network, 2)\n",
    "network = conv_2d(network, 64, 3, activation='relu')\n",
    "network = conv_2d(network, 64, 3, activation='relu')\n",
    "network = max_pool_2d(network, 2)\n",
    "network = fully_connected(network, 512, activation='relu')\n",
    "network = dropout(network, 0.5)\n",
    "network = fully_connected(network, 2, activation='softmax')\n",
    "network = regression(network,\n",
    "                     optimizer='adam',\n",
    "                     loss='categorical_crossentropy',\n",
    "                     learning_rate=0.001)\n",
    "\n",
    "# Training\n",
    "model = tflearn.DNN(network,tensorboard_verbose=3)\n",
    "model.fit(X, Y, \n",
    "          n_epoch=4,\n",
    "          snapshot_step=5, \n",
    "          show_metric=True,\n",
    "          validation_set=0.2,\n",
    "          batch_size=50,\n",
    "          shuffle=True,\n",
    "          run_id=\"audio-1\")\n",
    "\n",
    "model.save(\"shot_audio.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, Y_test = image_preloader(dataset_sample, \n",
    "                       image_shape=(32, 32),\n",
    "                       mode='file', \n",
    "                       categorical_labels=True,   \n",
    "                       normalize=True)\n",
    "\n",
    "Y_predict = model.predict(X_test[:])\n",
    "\n",
    "m = {}\n",
    "for i in range(len(Y_predict)):\n",
    "    pred_0, pred_1 = int(round(Y_predict[i][0])), int(round(Y_predict[i][1]))\n",
    "    test_0, test_1 = int(round(Y_test[i][0])), int(round(Y_test[i][1]))\n",
    "    key = \"{}-{}-{}-{}\".format(pred_0, pred_1, test_0, test_1)\n",
    "    if key == '1-0-1-0':\n",
    "        key = \"tn\"\n",
    "    elif key == '0-1-0-1':\n",
    "        key = \"tp\"\n",
    "    elif key == '0-1-1-0':\n",
    "        key = \"fp\"\n",
    "    elif key == '1-0-0-1':\n",
    "        key = \"fn\"\n",
    "    if key in m:\n",
    "        m[key] += 1\n",
    "    else:\n",
    "        m[key] = 0\n",
    "\n",
    "print m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
